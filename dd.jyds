# attempts to implement http://tsds.org/dd

# defaults are for 5.15 ftp://virbo.org/users/ali/crres_electric_field/CRRES_E_30s_0193.txt
resourceURI  = getParam( 'resourceURI',  'ftp://virbo.org/users/ali/crres_electric_field/CRRES_E_30s_0193.txt', 'example file to load' )
columns      = getParam( 'columns',      '1',                         'The data columns, and groups like (vectors) and [spectrograms]' )
timeColumns  = getParam( 'timeColumns',  '0',                                                              'the time colums, e.g. 0-5' )
timeFormat   = getParam( 'timeFormat',   '$Y+$m+$d+$H+$M+$S',                                      'timeFormat e.g. $Y+$m+$d+$H+$M+$S' )
timeUnits    = getParam( 'timeUnits',    ''                                   'units of time, like seconds since 1970-01-01T00:00:00Z' )
timeIntervals= getParam( 'timeIntervals','',                                                              'R/1993-01-01T00:00:00Z/P1D' )
columnLabels = getParam( 'columnLabels', '',                                                                  'labels for each column' )
columnIDs    = getParam( 'columnIDs',    '',                                                  'IDs (Java identifiers) for each column' )
columnUnits  = getParam( 'columnUnits',  '',                                                                   'units for each column' )
fillValues   = getParam( 'fillValues',   '',                                         'fill value or values for each column (or group)' )
groupIDs     = getParam( 'groupIDs',     '',                                                                      'ids for each group' )
group        = getParam( 'group',        '',                                                                        'group id to plot' )
column       = getParam( 'column',       '',                                                                       'column id to plot' )
exampleConfig= getParam( 'exampleConfig', 'sampex', 'examples for debugging', [ '', 'sj', '5_10', 'ali1', 'ali2', 'sampex' ] )
debug=         getParam( 'debug', 1, 'turn debugger on', [ '0', '1' ] )

if ( exampleConfig=='ali1' ):
  resourceURI  = 'ftp://virbo.org/users/ali/crres_electric_field/CRRES_E_30s_0193.txt'
  columns      = '(6-8),9,(10-12),13,(14:17),(17-19),20'
  timeColumns  = '0-5'                    
  timeFormat   = '$Y+$m+$d+$H+$M+$S'
  timeUnits    = ''                                
  columnLabels = '(Ex,Ey,Ez),|E|,(Ex,Ey,Ez),|E|,(Vx,Vy,Vz),(Vx,Vy,Vz),Angle'
  columnUnits  = '(mV/M),mV/M,(mV/M),mV/M,(km/s),(km/s),deg'       
  fillValues   = '(99999),99999,(99999),99999,(99999),(99999),99999'
  groupIDs     = 'EGSESpherical,EGSECylindrical,VGSESpherical,VGSECylindrical'
  group        = 'EGSESpherical'
  
elif ( exampleConfig=='5_10' ):
  resourceURI  = 'http://magweb.cr.usgs.gov/data/magnetometer/BDT/OneMinute/bdt20110914vmin.min'
  columns      = '2,(3-5),6'
  timeColumns  = '0-1'                    
  timeFormat   = '$Y-$m-$d+$H:$M:$S'
  timeUnits    = ''                                
  columnLabels = 'DOY,(BDTH,BDTD,BDTZ),BDTF'
  columnUnits  = 'DOY,(nT),nT'       
  fillValues   = '999,(99999),99'
  groupIDs     = ''
  group        = ''

elif ( exampleConfig=='sj' ):
  # http://sarahandjeremy.net/~jbf/1wire/8000080044015010.dat?timeUnits=seconds%20since%201970-01-01&columnUnits=degF&fillValues=-9999
  resourceURI  = 'http://sarahandjeremy.net/~jbf/1wire/8000080044015010.dat'
  columns      = '1'
  timeColumns  = '0'                    
  timeFormat   = ''
  timeUnits    = 'seconds since 1970-01-01T00:00'                                
  columnLabels = 'temperature'
  columnUnits  = 'degF'       
  fillValues   = '-9999'
  groupIDs     = ''
  group        = ''

elif ( exampleConfig=='ali2' ):
  resourceURI  = 'ftp://virbo.org/users/ali/crres_ephemeris/CRRES_EPH_5m_0190.txt'
  timeColumns  = '0-5'
  timeFormat   = '$Y+$m+$d+$H+$M+$S'
  columns      = '6,7,8,9,10,(11-13),(14-15),(16-17)'
  columnLabels = 'CRRES spacecraft orbit number,CRRES magnetic local time position,CRRES magnetic latitude position,CRRES radial distance position,CRRES spacecraft L shell value,(X,Y,Z),(NLONG,NLAT),(SLONG,SLAT)'
  columnIDs    = 'OrbitNumber,MLT,MLat,R,L,(Position),(NLONG,NLAT),(SLONG,SLAT)'
  columnUnits  = ',,degrees,Re,,(Re),(degrees),(degrees)'
  groupLabels  = 'CRRES spacecraft positional Vector in ECI coordinates, CRRES spacecraft northern hemisphere footprint location, CRRES spacecraft southern hemisphere footprint location'
  groupIDs     = 'Position,North,South'
  group= 'North'
  fillValues   = '99999'

elif ( exampleConfig=='sampex' ):
  resourceURI  = 'http://autoplot.org/data/sampex.dat'
  columns      = '[0:90]'   # consider '[:]'
  columnLabels = '(Flux)'
  columnLabelValues = '(1.1:10.0:0.1)'
  timeIntervals= 'R/1993-01-01T00:00Z/P1D'
  timeUnits= ''
  timeFormat= ''
  groupIDs  = ''
  group= ''
  
# the goal with this is to create a bundle descriptor for the columns.  We will do this using SparseDataSet,
# a new dataset type ideal for creating bundle descriptors.

from org.virbo.dataset import BundleDataSet
from org.virbo.dataset import SparseDataSet, SparseDataSetBuilder
from org.virbo.dataset import DataSetOps
from org.virbo.dataset import SemanticOps

bdsb= SparseDataSetBuilder(2)

from java.util.regex import Pattern

# (1.0:10.0:0.1) -> 1.0,1.1,...,10.0
# (999) -> 999,999,...,999
# (1,2,3) -> 1,2,3
def unpackValues( v, count ):
   if ( v[0]=='(' or v[0]=='[' ):
      v= v[1:-1]  # pop off the ()s
   if ( v.find(',')>-1 ):
      sresult= v.split(',')
      result= [ float(v1) for v1 in v.split(',') ]  # jython map
   elif( v.find(':')>-1 ):
      vs= v.split(':')
      start= float(vs[0])
      stop= float(vs[1])
      step= float(vs[2])
      n= int( (stop-start) / step )
      # note N might not equal count.  We allow for some sloppiness because rounding...
      result= taggen( start, step, count, Units.dimensionless ) 
   else:
      result= [float(v)]*count
   if ( len(result)!=count ):
      raise Exception( 'expected %d elements in %s, got %d' % ( count,v,len(result) ) )
   return result

def unpackUnits( v, count ):
   if ( len(v)>0 and v[0]=='(' ): v= v[1:-1]  # pop off the ()s
   if ( v.find(',')>-1 ):
      result= map( SemanticOps.lookupUnits, v.split(',') )
      if ( len(result)!=count ):
         raise Exception( 'expected %d elements in %s, got %d' % ( count,v, len(result) ) )
   else:
      result= [SemanticOps.lookupUnits(v)]*count
   return result

def unpackLabels( v, count ):
   if ( v[0]=='(' ): v= v[1:-1]  # pop off the ()s
   if ( v.find(',')>-1 ):
      result= v.split(',')
      if ( len(result)!=count ):
         raise Exception( 'expected %d elements in %s' % ( count,v ) )
   else:
      result= [v]*count
   return result
   
rang=  '[\(\[]\d+[\-]\d+[\)\]]'             # build up the regex for columns
xrang= '[\(\[]\d+[\:]\d+[\)\]]'
lis= '\(\d+(,\d+)*\)'
one= '(\d+|%s|%s|%s)' % ( rang, xrang, lis )
pat= '%s((,%s)*)' % ( one, one )   #  "pattern"
p= Pattern.compile( pat )
rangp= Pattern.compile( rang )
xrangp= Pattern.compile( xrang )
lisp= Pattern.compile( lis )

gitems= '[\(\[][^(^)^,]*(,[^(^)^,]*)*\)'
gone= '[^(^,^)]*|%s' % gitems
gpat= '(%s)((,(%s))*)' % ( gone, gone )
gp= Pattern.compile( gpat )   # "general pattern"

unpackColumns=[]
mcolumns= p.matcher(columns)
mcolumnLabels= gp.matcher(columnLabels)
if ( len(columnIDs)>0 ):
   mcolumnIDs= gp.matcher(columnIDs)
else:
   mcolumnIDs= None
mcolumnUnits= gp.matcher(columnUnits)
if ( len(fillValues)>0 ):
   mfillValues= gp.matcher(fillValues)
else:
   mfillValues= None
   
mgroupIDs= gp.matcher(groupIDs)
if ( len(columnLabelValues)>0 ):
   mcolumnLabelValues= gp.matcher(columnLabelValues)
else:
   mcolumnLabelValues= None
   
mgroupIDs= gp.matcher(groupIDs)


icol= 0 # column of result dataset
cont= mcolumns.matches() 

if ( not cont ):
   raise Exception('initial columns match failure')
if ( not mcolumnLabels.matches() ): 
   raise Exception('initial columnLabels match failure')
if ( mcolumnIDs!=None and not mcolumnIDs.matches() ): 
   raise Exception('initial columnIDs match failure')
if ( not mcolumnUnits.matches() ): 
   raise Exception('initial columnUnits match failure')
if ( mcolumnLabelValues!=None and not mcolumnUnits.matches() ): 
   raise Exception('initial columnLabelValues match failure')
if ( mfillValues!=None and not mfillValues.matches() ): 
   raise Exception('initial fillValues match failure')

# we allow one fill value for the whole file.   
notConstantFill= True

from org.das2.util import LoggerManager
from java.util.logging import Level
logger= LoggerManager.getLogger('autoplot.dd')
logger.setLevel( Level.ALL )
logger.warning('test of warning')
logger.info('test of info')
logger.fine('test of fine')

# count the number of items.  If there is just one item, then we will use it automatically.
itemCount= 0

while ( cont ):
   if ( not mcolumnLabels.matches() ): 
       raise Exception('columnLabels match failure')
   if ( mcolumnIDs!=None and not mcolumnIDs.matches() ): 
       raise Exception('columnIDs match failure')
   if ( mcolumnLabelValues!=None and not mcolumnLabelValues.matches() ): 
       raise Exception('columnLabelValues match failure')
   if ( not mcolumnUnits.matches() ): 
       raise Exception('columnUnits match failure')
   if ( mfillValues!=None and not mfillValues.matches() ): 
       raise Exception('fillValues match failure')
   
   ths1= mcolumns.group(1)
      
   if ( debug ):
      print '----'
      print 'columns=', mcolumns.group(1)
      print 'columnLabels=', mcolumnLabels.group(1)
      if ( mcolumnIDs!=None ): print 'columnIDs=', mcolumnIDs.group(1)
      print 'columnUnits=', mcolumnUnits.group(1)
      if ( mfillValues!=None ): print 'fillValues=', mfillValues.group(1)
      if ( mcolumnLabelValues!=None ): print 'mcolumnLabelValues=', mcolumnLabelValues.group(1)
      
   if ( ths1[0]=='(' or ths1[0]=='[' ):
      if ( not mgroupIDs.matches() ): 
         raise Exception('groupIDs match failure')
      groupId= mgroupIDs.group(1)
      if ( debug ): print 'groupID='+groupId
      count=0
      logger.log( Level.FINE, 'ths1: '+ths1 )  
      logger.log( Level.FINE, 'rangp: '+rangp.toString() )  
      logger.log( Level.FINE, 'ths1: '+ths1 )  
      logger.log( Level.FINE, 'xrangp: '+xrangp.toString() )  
      if ( rangp.matcher(ths1).matches() and ths1.find('-')>-1 ):     # (6-8)
         i= ths1.index('-')
         st= int(ths1[1:i])
         en= int(ths1[i+1:-1])
         for i in xrange(st,en+1): unpackColumns.append(i)
         en= en+1
         count= en-st
         logger.log( Level.FINE, 'count1: %d' % count )   
      elif ( xrangp.matcher(ths1).matches() ):  #(6:9)  # TODO: (6:11:2)
         i= ths1.index(':')
         st= int(ths1[1:i])
         en= int(ths1[i+1:-1])
         for i in xrange(st,en): unpackColumns.append(i)
         count= en-st
         logger.log( Level.FINE, 'count2: %d' % count )   
      elif ( lisp.matcher(ths1).matches() ):    #(6,7,8)
         ss= ths1.split(',')
         st= int(ss[0])
         for s in ss: 
            unpackColumns.append(int(s))
            count= count+1
         logger.log( Level.FINE, 'count3: %d' % count )       
      else: 
         raise Exception("roo?")
      labels= unpackLabels( mcolumnLabels.group(1), count )
      if ( mcolumnIDs==None ):
         names= labels
         names= [ safeName(s) for s in names ]
      else:
         names= unpackLabels( mcolumnIDs.group(1), count )
      if ( mfillValues!=None ): fills= unpackValues( mfillValues.group(1), count )
      if ( mcolumnLabelValues!=None ): dep1Values= unpackValues( mcolumnLabelValues.group(1), count )
      units= unpackUnits( mcolumnUnits.group(1), count )
      if ( ths1[0]=='(' ):
         for i in xrange(count):
           if ( i==0 ): bdsb.putValue( icol+i,0, count )
           bdsb.putProperty( QDataSet.LABEL, icol+i, labels[i] )
           bdsb.putProperty( QDataSet.NAME, icol+i, names[i] )
           bdsb.putProperty( QDataSet.UNITS, icol+i, units[i] )
           if ( mfillValues!=None ): bdsb.putProperty( QDataSet.FILL_VALUE, icol+i, fills[i] )
           bdsb.putProperty( QDataSet.ELEMENT_NAME, icol+i, groupId )
           bdsb.putProperty( QDataSet.START_INDEX, icol+i, icol )
      else:
         for i in xrange(count):
           if ( i==0 ): bdsb.putValue( icol+i,0, count )
           if ( mcolumnLabelValues!=None ): 
              bdsb.putProperty( QDataSet.DEPEND_1, icol+i, dep1Values )
           bdsb.putProperty( QDataSet.LABEL, icol+i, labels[i] )
           bdsb.putProperty( QDataSet.NAME, icol+i, names[i] )
           bdsb.putProperty( QDataSet.UNITS, icol+i, units[i] )
           if ( mfillValues!=None ): bdsb.putProperty( QDataSet.FILL_VALUE, icol+i, fills[i] )
           bdsb.putProperty( QDataSet.ELEMENT_NAME, icol+i, groupId )
           bdsb.putProperty( QDataSet.START_INDEX, icol+i, icol )
           bdsb.putProperty( QDataSet.RENDER_TYPE, icol+i, 'spectrogram' )
      icol= icol+count   
   else:
      unpackColumns.append(int(ths1))
      groupId= None
      bdsb.putProperty( QDataSet.LABEL, icol, mcolumnLabels.group(1) )
      if ( mcolumnIDs==None ):
          bdsb.putProperty( QDataSet.NAME, icol, safeName(mcolumnLabels.group(1)) )
      else:
          bdsb.putProperty( QDataSet.NAME, icol, safeName(mcolumnIDs.group(1)) )
      bdsb.putProperty( QDataSet.UNITS, icol, SemanticOps.lookupUnits( mcolumnUnits.group(1) ) )
      if ( mfillValues!=None ):
          bdsb.putProperty( QDataSet.FILL_VALUE, icol, float(mfillValues.group(1)) )
      bdsb.putProperty( QDataSet.ELEMENT_NAME, icol, groupId )
      icol= icol+1

   if ( mcolumns.group(3)!=None ):  
       # advance to next group
       mcolumns= p.matcher( mcolumns.group(3)[1:] )  
       mcolumnLabels= gp.matcher( mcolumnLabels.group(3)[1:] )
       if ( mcolumnIDs!=None ): mcolumnIDs= gp.matcher( mcolumnIDs.group(3)[1:] )
       mcolumnUnits= gp.matcher( mcolumnUnits.group(3)[1:] )
       if ( mcolumnLabelValues!=None ): mcolumnLabelValues= gp.matcher( mcolumnLabelValues.group(3)[1:] )
       if ( mfillValues!=None ):
          if ( notConstantFill and mfillValues.group(3)!=None and len(mfillValues.group(3))>0):
             mfillValues= gp.matcher( mfillValues.group(3)[1:] )
          else:
             notConstantFill= False
             logger.fine( 'assuming one fill value' )
       if ( groupId!=None ): 
          newt= mgroupIDs.group(3)[1:]
          mgroupIDs= gp.matcher( newt )
       cont= mcolumns.matches()
   else:
       cont= False
       
   itemCount= itemCount+1
   
   if ( not cont ):
       if ( mgroupIDs.matches() and len(mgroupIDs.group(1))>0):     
           raise Exception('extra groupIDs: '  + mgroupIDs.group(1) )
       if ( mfillValues!=None and mfillValues.matches() and len(mfillValues.group(1)) and notConstantFill ):   raise Exception('extra fillValues: ' +mfillValues.group(1) )
       if ( mcolumnLabels.matches() and len(mcolumnLabels.group(1)) ): raise Exception('extra labels: '   +  mcolumnLabels.group(1) )
       if ( mcolumnUnits.matches() and len(mcolumnUnits.group(1)) ):  raise Exception('extra units: '   +   mcolumnUnits.group(1) )

bdsb.setLength( icol )
bds= bdsb.getDataSet()

if ( resourceURI[-1]=='/' ):
   raise Exception('resourceURI must be a file')
f= getFile( resourceURI,monitor )

from org.virbo.dsutil import AsciiParser,MultiFieldTimeParser
ap= AsciiParser()
delimParser= ap.guessSkipAndDelimParser( f.toString() )

logger.finest( 'timeIntervals=' + timeIntervals )
if ( timeFormat!='' ):
   tfs= timeFormat.split('+')
   mftp= MultiFieldTimeParser.create( ap, 0, tfs )
elif ( timeUnits!='' ):
   ap.setUnits( 0, SemanticOps.lookupTimeUnits( timeUnits ) )
elif ( timeIntervals!='' ):
   logger.finest( 'timeIntervals' )
else:
   logger.fine( 'hope that the first column is iso8601' )

rank2= ap.readFile( f.toString(), monitor )

if ( timeFormat!='' ):
   tt= mftp.unpack( rank2, mftp.getUnits() )
elif ( timeIntervals!='' ):
   ss= '/'.split( timeIntervals[2:] )
   tr1= DatumRangeUtil.parseTimeRange(timeIntervals[2:])
   logger.fine( 'tr1: ' + tr1.toString() )
   tt= timegen( tr1.min().add( tr1.width().divide(2) ).toString(), tr1.width().toString(), rank2.length() )   
else:
   tt= DataSetOps.unbundle( rank2, 0 )
   tt.putProperty( QDataSet.UNITS, SemanticOps.lookupTimeUnits( timeUnits ) ) #why?

rank2.putProperty(QDataSet.DEPEND_0,tt)

if ( len( unpackColumns )==0 ):
   raise Exception("no data found to unpack")
   
result= rank2[:,unpackColumns]
result.putProperty(QDataSet.BUNDLE_1,bds)
#peekAt(bds)

if ( group!='' ):
    logger.log( Level.FINE, 'unpacking group '+group )
    result= DataSetOps.unbundle( result, group )
    result.putProperty(QDataSet.DEPEND_0,tt)  # why must I do this?
elif ( itemCount==1 ):
    logger.log( Level.FINE, 'unpacking default group because there is just one' )
    result= DataSetOps.unbundle( result, 0, True )
    result.putProperty(QDataSet.DEPEND_0,tt)  # why must I do this?
else:
    logger.log( Level.FINE, 'leaving dataset alone.  unpackColumns= %d' % len(unpackColumns) )

logger.fine( 'result='+result.toString() )
